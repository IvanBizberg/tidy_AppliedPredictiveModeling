---
title: "Applied Predictive Modeling"
author: "Ivan Bizberg"
date: "7/28/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# libraries
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidymodels)
library(tidyverse)
library(tidyquant)
library(magrittr)
library(caret)

library(AppliedPredictiveModeling)
```


# Part I General Strategies
## 3. Preprocessing


data exploration
```{r warning=FALSE}
library(AppliedPredictiveModeling) # import data
data(segmentationOriginal)
train = segmentationOriginal %>% filter(Case == "Train") %>% select(-Cell, -Case, -contains("Status")) 

# Check correlation
train %>% select(-Class) %>% cor() %>% corrplot::corrplot(., order = "hclust")
train %>% select(-Class) %>% cor() %>% caret::findCorrelation(cutoff = 0.75) #%>% length()
# Check near-zero variance
train %>% caret::nearZeroVar() # no predictor with near zero variance
# Check skewness 
skewvars = train %>% select(-Class) %>% sapply(skewness) %>% stack() %>% filter(values > 2 | values < -2) %>% pull(., ind) # A skewness between -2 and 2 is okay

ggplot(train, aes(VarIntenCh1)) + geom_histogram(fill = "red", alpha = 0.2) # assess visually the shape of the distribution
```


Fix the skewness by tranforming the data with boxcox
```{r}
tidy_rec = recipe(Class ~ ., data = train) %>% 
  step_BoxCox(all_of(skewvars)) %>% # skew transformation before scale and center
  step_center(all_numeric()) %>% 
  step_scale(all_numeric()) %>%
  step_nzv(all_predictors()) %>% 
  step_corr(all_predictors(), threshold = 0.75) %>% # to remove correlated variables or
  step_pca(all_numeric(), num_comp = ncol(train), id = "pca") #%>%  #to remove colinearity do pca
  #prep() # or
tidy_prep = prep(tidy_rec) 

trainProcessed = juice(tidy_prep)

tidy_prep %>% tidy(id = "pca", type = "coef") %>% # check loading with "coef" 
  select(-id) %>% 
  pivot_wider(names_from = component, values_from = value) %>% 
  ggplot(aes(PC1, PC2, color = terms)) + geom_point() # Loadings close to zero indicate that the predictor variable did not contribute much to that component.

tidy_prep %>% tidy(id = "pca", type = "variance") %>% # check how much variance does each component account for with "variance"
  select(-id) %>% mutate(value = round(value, digits = 3)) %>% 
  pivot_wider(names_from = terms, values_from = value) 

trainProcessed %>% ggplot(aes(PC02, PC03, color = Class)) + geom_point()

trainProcessed

dim(trainProcessed)

ggplot(trainProcessed, aes(PC01)) + geom_histogram(fill = "red", alpha = 0.2) # Check if fixed skewness

```

How to create dummy variables in tidymodels

```{r}
# Import the first 900 rows form csv
library(data.table)
file <- "C:/Users/ivan/Google Drive/Machine learning/Kaggle/Used Cars Dataset/vehicles.csv"
minicars = fread(file, nrows = 20) 
minicars %<>% select(-description, -contains("url")) %>% select(price, model, odometer)
names(minicars)
```



```{r}
carsprocessed = recipe(price ~ ., minicars) %>% 
  step_naomit(everything()) %>% 
  step_interact(~ model : odometer) %>%
  step_dummy(all_nominal(), one_hot = T) %>%
  prep() %>% juice()
  

```

### Exercices chap 3

Exo 1
```{r}
library(mlbench)
data(Glass)
str(Glass)
```

a) explore data
```{r}
# See distribution
Glass %>% pivot_longer(-Type, names_to = "elements", values_to = "percentages") %>% 
  ggplot(aes(percentages, fill = elements)) + geom_density() + facet_wrap(~ elements, scales = "free")
# we need center and scale data to much difference between elements, we can already see that there are skewness in 3 elements and need to be transform.
# heavy tailed distribution in Ca, Na, Si, Ri: they have a high concentration of point in the center of the scale and a small number of points at the edges



# See relationship between elements
Glass %>% select(-Type) %>% cor() %>% corrplot::corrplot(order = "hclust", method = "number", type = "upper") 
library("GGally")
Glass %>% select(-Type) %>% ggcorr(label = T)
Glass %>% select(-Type) %>% ggpairs()
# There is only one high correlation between elements Rl and Ca (positive corr)

```
b) search for outliers, skwness and near zero variance
```{r}
Glass %>% pivot_longer(-Type, names_to = "elements", values_to = "percentages") %>% 
  ggplot(aes(elements, percentages, fill = elements)) + geom_boxplot(alpha = 0.6) + facet_wrap(~ elements, scale = "free")
# Yes it seem there are outliers especially one point in Fe (need models resistant to outliers)

skew = Glass %>% select(-Type) %>% sapply(skewness) %>% stack() %>% filter(values > 1.5 | values < -1.5) %>% pull() %>% droplevels()
# Yes skewness in K, Ca, Ri and Ba

Glass %>% caret::nearZeroVar()
# No
```
c) We are going to center, scale and fix skewness by transforming the predictors with yeo-johnson familly that is similar a box-cox (box-cox and log are not posible because some predictor have zeros)
```{r}
Glass %>% glimpse() 
tidy_rec = Glass %>% recipe(Type ~ .) %>% 
  step_YeoJohnson(all_of(skew)) %>% 
  step_scale(all_numeric()) %>% 
  step_center(all_numeric()) %>% 
  step_spatialsign(all_numeric()) %>% # turn on and off to see if yeo-johson transformation improved data
  prep()

tidy_rec %>% juice() %>% pivot_longer(-Type, names_to = "elements", values_to = "percentages") %>% 
  ggplot(aes(percentages, fill = elements)) + geom_density(alpha = 0.6) + facet_wrap(~ elements, scale = "free")


tidy_rec %>% juice() %>% ggpairs()
# The tranformation yeo - johson dosn't seem to improve the data in terms of skweness. So under these kinds of circumstances, we will need to use models that are not unduly affected by skewed distributions (e.g. tree-based methods).
# The tranformation spatial sign did minimize the number of unusually extreme observations
```

Exo 2
a)
```{r}
library(mlbench)
data(Soybean)
?Soybean
Soybean %>% glimpse() 
Soybean %>% mutate_at(vars(-Class),as.numeric) %>% #glimpse()
  pivot_longer(-Class, names_to = "predictors", values_to = "values") %>% 
  ggplot(aes(values, fill = predictors)) + geom_histogram() + facet_wrap(~ predictors) +
  guides(fill = F)
Soybean %>% count(hail)

```
b)
```{r}

colSums(is.na(Soybean)) %>% stack() %>% arrange(desc(values))
Soybean %>% mutate(sum.na = rowSums(is.na(Soybean))) %>% group_by(Class) %>% summarise(sum_na = sum(sum.na)) %>% 
  arrange(desc(sum_na))
table(Soybean$Class, complete.cases(Soybean))

Soybean %>% mutate_all(as.character) %>% 
  mutate(date = recode(date, "0" = "apr", "1" = "may", "2" = "june", "3" = "july", 
                       "4" = "aug", "5" = "sept", "6" = "oct")) %>% 
  mutate(date = fct_explicit_na(date)) %>% 
  ggplot(aes(date, fill = temp)) + geom_bar() + coord_flip()

n = Soybean %>% filter(!complete.cases(.)) %>% map(~ mean(is.na(.))) %>% as.tibble() %>% mutate_all(as.character)
Soybean %>% filter(!complete.cases(.)) %>% bind_rows(n)
  bind_rows(summarise_all(colSums(is.na(.))))
  add_row(colSums(is.na(.)))
  mutate(sum.na = colSums(is.na(.))) %>% 
  group_by(Class) %>% summarise(sum_na = sum(sum.na)) %>% 
  arrange(desc(sum_na))
  
# We can see that some predictor are 100% missing values son imputation tecniques can not work 
# We could encode the missing as another level or eliminate the classes associated with the high rate of missing values from the data altogether.


```
c)
```{r}
# We can see that some predictor are 100% missing values son imputation techniques can not work 
# We could encode the missing as another level or eliminate the classes associated with the high rate of missing values from the data altogether.
# Depending of the model used sparsity (frequency of the predictor values) can be an issue, tree or rule based models, or naive Bayes can work without the need to remove sparse dummy variables
# To check how bad sparsity is we can convert factors in dummy variables and test with near zero variance

tidy_rec = Soybean %>% mutate_all(as.character) %>% mutate_all(., fct_explicit_na) %>% 
  # sapply(levels)
  recipe(Class ~ .,) %>% 
  step_dummy(all_predictors()) %>% 
  step_nzv(all_predictors()) %>%
  prep() %>% juice()

tidy_rec %>% tidy(., number = 2) # 16 dummy variables need to be remove
```
Exo3 
```{r}
library(caret)
data(BloodBrain)
BloodBrain = bbbDescr %>% bind_cols(logBBB) %>% rename(outcome = ...135) %>% glimpse()
```
a)
```{r}
# We look at the distributions of the remaining predictors (or a leat a random sample of them)
set.seed(532)

Mini_BloodBrainNZV = BloodBrain %>% names(.) %>% as.tibble() %>% sample_n(size = 8) %>% pull()
BloodBrain %>% #select(Mini_BloodBrainNZV) %>% 
  select(wpsa2, fpsa3) %>% 
  # select(-outcome) %>% 
  pivot_longer(names(.), names_to = "predictors", values_to = "values") %>% 
  ggplot(aes(values, fill = predictors)) + geom_density(alpha = 0.5) + facet_wrap(~ predictors, scales = "free") +
  guides(fill = F)
# Some predictors exhibit skewness and some shows two distinct modes.



# Check skewness predictors that need to be tranform yeo-johnson because some predictors are negative
vars_skew = BloodBrain %>% sapply(skewness) %>% stack() %>% filter(values < -2 | values > 2) %>% pull()

# Preprocess
BloodBrain_rec = BloodBrain %>% recipe(outcome ~ .) %>% 
  step_YeoJohnson(all_of(vars_skew)) %>% 
  step_nzv(all_predictors()) %>% 
  step_spatialsign(all_predictors()) %>% 
  step_corr(all_predictors(), threshold = 0.9) %>% 
  prep() 

BloodBrain_rec %>% tidy(number = 3)
BloodBrain %>% count(alert)

BloodBrain_post = BloodBrain_rec %>% juice()


Mini_BloodBrainNZV = BloodBrain_post %>% names(.) %>% as.tibble() %>% sample_n(size = 9) %>% pull()
BloodBrain_post %>% select(wpsa2, fpsa3) %>% 
  # select(-outcome) %>% 
  pivot_longer(names(.), names_to = "predictors", values_to = "values") %>% 
  ggplot(aes(values, fill = predictors)) + geom_density(alpha = 0.5) + facet_wrap(~ predictors, scales = "free") +
  guides(fill = F)

# Although the distributions for fpsa3 and wpsa2 are more symmetric, the other predictors have either additional modes or more pronounced modes. One option would be to manually assess which predictors would benefit from this type of transformation.
```
b)
```{r}
# We now need to check correlation\
# The correlation matrix of the predictors can be computed and examined. However, we know that many predictors are skewed in these data. Since the correlation is a function of squared values of the predictors, the samples in the tails of the predictor distributions may have a significant effect on the correlation structure. For this reason, we will look at the correlation structure three ways: the untransformed data, the data after the Yeo{Johnson transformation, and the data after a spatial sign transformation.
library(corrplot)
BloodBrain_post %>% ggcorrplot::ggcorrplot()

CorSS = cor(BloodBrain_post)
CorSS %>% corrplot(order = "hclust")
# This visualization indicates that correlations lessen with increasing levels of transformations:


# another way to remove correlation in a less greedy way is to use the subselect library and use simulated annealing and genetic algorithms to search for quality subsets

library(subselect)
rawCorr = cor(BloodBrain_post)
trimmed <- trim.matrix(rawCorr, tolval=1000*.Machine$double.eps)$trimmedmat
ncol(trimmed)
set.seed(702)
sa <- anneal(trimmed, kmin = 18, kmax = 18, niter = 1000)
saMat <- rawCorr[sa$bestsets[1,], sa$bestsets[1,]]

```

## 4. Model Tuning

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Import data
library(caret)
data(GermanCredit)

GermanCredit %>% glimpse()
``` 


## Spliting data

```{r}
set.seed(1056)
split = initial_split(GermanCredit, prop = 0.8)
train = training(split)
test = testing(split)
```


## Preprocessing
```{r}
train %>% count(Personal.Female.Single)


train %<>% 
  mutate(Class = as.character(Class)) %>%
  mutate_if(is.double, as.integer) %>% 
  select(-Personal.Female.Single, -Purpose.Vacation) %>% 
  glimpse()


tidy_rec = recipe(Class~ ., 
                  data = train) %>% 
  step_scale(all_numeric()) %>% 
  step_center(all_numeric()) %>% 
  prep()
  
  
  
Pre_data = tidy_rec %>% juice()
Pre_data %>% glimpse()

```


## Tunning hyperparameters
```{r}
kfold_cv = vfold_cv(train, v = 10, repeats = 5)

tidy_svm = svm_rbf(mode = "classification",
                   cost = tune(),
                   margin = NULL,
                   rbf_sigma = NULL) %>% 
  set_mode("classification") %>% 
  set_engine("kernlab")


tidy_wf = workflow() %>% 
  add_recipe(tidy_rec) %>% 
  add_model(tidy_svm)

library(doParallel)
all.cores = detectCores(logical = F)
cl = makePSOCKcluster(all.cores)
registerDoParallel(cl)

tidy_tune = tidy_wk %>% 
  tune_grid(resamples = kfold_cv,
  metrics = metric_set(accuracy, kap))

```
###Plot evaluation
```{r}
tidy_tune %>% collect_metrics()
tidy_tune %>% show_best("accuracy")
best_tune = tidy_tune %>% select_best("accuracy")

autoplot(tidy_tune)
```

### Evaluate on test set
```{r}
Last_wf = tidy_wf %>% finalize_workflow(best_tune)

svm_fit = Last_wf %>% last_fit(split)
svm_fit %>% collect_metrics() # Evaluation
```


### Predict
```{r}
tidy_predict = svm_fit %>% collect_predictions()

tidy_predict %>% conf_mat(truth = Class, estimate = .pred_class)
```

### Extract last model for deployment
```{r}
Final_model = fit(Last_wf, GermanCredit)


saveRDS(Final_model, "SVMCredit.rds")
```

### Re-import model  
```{r}
ModelSVMCredit = read_rds("SVMCredit.rds")
Model = predict(ModelSVMCredit, GermanCredit)
```

## Between model comparaison 

```{r}

```

### Excercices chap 4

Exo1

What data splitting method(s) would you use for Music Genre data set.
When determining the data splitting method, we should focus on two primary characteristics:
. the number of samples relative to the number of predictors in the data
. the distribution of samples across classes.

12495/191 = 65
For these data, the number of samples is 12495 and the number of predictors is 191. Because the number of samples is more than 65 fold greater than the number of predictors, we likely can split the data into a training set and test set. This split would enable us to independently verify model
performance results without impacting the estimation of the optimal tuning parameter(s) selection.

While there is a distinct imbalance, the number of samples in the overall data set is large enough such that resampling or cross-validation techniques have a good chance at randomly selecting samples across all classes in similar proportion to the entire data set.

=> k-fold cross-validation, with small k (5-10)


```{r}
set.seed(31)
split %>% initial_split(data, prop = 0.8)
train = training(split)
test = testing(split)
kfold = vfold_cv(train, v = 10, strata = NULL)
```

Exo2

```{r}
data(permeability) 
data = fingerprints %>% as.data.frame() %>% 
  bind_cols(permeability) %>% rename(permeability = ...1108)
dim(data) # 165/1108 = 0.14 we are going to need to reduce the dimensions

data %>% ggplot(aes(permeability)) + geom_histogram(binwidth = 10)


kfolds = vfold_cv(data, v = 10, repeats = 25, strata = permeability, breaks = 10)
kfolds$splits[[46]] %>% as.data.frame() %>% ggplot(aes(permeability)) + geom_histogram(binwidth = 10)
```
Due to the small number of samples the data shouldn't be split in test data and the resample technique should be stratified repeated 10-fold cross validation.

Clearly, the distribution of permeability values is skewed, with the majority of samples having low values. Because the overall number of samples is small, randomly partitioning the samples may create sets that are not representative of the overall data set. That is, we are likely to have many partitions of the data that contain relatively few of the larger permeability samples.

To create more representative, but still random, selections we should use stratification.


Exo3

```{r}
library(plsmod)
data(ChemicalManufacturingProcess)

data = ChemicalManufacturingProcess
data %>% glimpse()

# Data too small to split data in test/train 

# Validation for tuning number of components in PLS doesn't work the normal way
kfolds = data %>% vfold_cv(v = 10, repeats = 5)
tidy_rec = data %>% recipe(Yield ~ .) %>% 
  step_scale(all_predictors()) %>% 
  step_center(all_predictors()) 


tidy_model = pls(num_comp = tune()) %>%
  set_engine("mixOmics") %>%
  set_mode("regression")


tidy_wf = workflow() %>% 
  add_recipe(tidy_rec) %>% 
  add_model(tidy_model)


library("doFuture")
all_cores <- parallel::detectCores(logical = FALSE) 
cl <- makeCluster(all_cores)
plan(cluster, workers = cl)

tidy_param = tidy_wf %>% parameters() %>% 
  update(num_comp = num_comp(range = c(2, 10)))



tidy_tune = tidy_wf %>% 
  tune_grid(resamples = kfolds,
            param_info = tidy_param,
            metrics = metric_set(rsq, rmse))



tidy_tune %>% collect_metrics() %>%
  filter(.metric == "rsq") %>% 
  ggplot(aes(num_comp, mean)) + geom_point() +
  geom_linerange(aes(ymax = mean + std_err, ymin = mean - std_err))



# a) The best setting uses 3 PLS components with a lower bound of 0.525. If the R2 is equal to the SD bound and is simpler (less parameters, 2 or 1) then we can argue that the simpler model is the most optimal 

```

```{r}
# b)
# lets now plot the tolerance values 
tidy_tune %>% collect_metrics() %>%
  filter(.metric == "rsq") %>% 
  mutate(tolerance = (mean - max(mean))/max(mean)*100) %>% 
  ggplot(aes(num_comp, tolerance)) + geom_point() 
# The lowest setting that does not exceed a 10% tolerance is a 2 component model.

# c and d) 
# Even if the best model from the R2 perspective is the random forest the model that optimize R2 is SVM because it need less computational time and the confident intervals have some overlap
# If the prediction function needed to be recoded for use, neither of these models would be preferred. In that case, the regression tree or PLS model would be better choices, albeit with a substantial drop in R2.
```


Exo4
```{r}
library(caret)
data(oil)
data = fattyAcids %>% bind_cols(oilType) %>% rename(oilType = ...8)

```
a)
```{r}
data %>% count(oilType)
data %>% sample_n(60) %>% 
  ggplot(aes(oilType)) + geom_bar()
# no the random sample match the original sample some time we loose some oil types

```
b)
```{r}
set.seed(629)
kfolds = data %>% vfold_cv(v = 20, strata = oilType)

SeeFolds = function(t){
  kfolds$splits[[t]] %>% as.data.frame() %>% count(oilType) 
}
SeeFolds(t = 4)

```


c)
Initial split into test/train isn't recommended because of the small sample size. It could be done if we only consider the classes with the most samples A and B and maybe E and F although this would only protect against gross overfitting

One possibility would be leave oneout cross-validation only because, with the exception of class G, each class will be represented in each re-sample. It should be noted that some classification models require at least one sample from each class, so resampling these data may place a restriction one which models can be used. 

d)
```{r}
# Understanding the uncertainty of a test set
binom.test(10, 20) # 10 correct prediction out of 20 (20 = test set)

# Width of a binomial confident interval = 0.66- 0.33 = 0.33



binom.test(40, 80)
# Confident interval = 0.61- 0.38 = 0.23


binom.test(160, 320) %>% tidy()
# Confident interval = 0.55- 0.44 = 0.11

Width = function(correct, test){
  binom.test(correct, test) %>% tidy() %>% mutate(width = conf.high - conf.low) %>% select(width) %>% pull()
}

grid = expand.grid(correct = 0:20, test = 0:20)
ciWidth = t(apply(grid, 1, Width)) 
ciWidth %>% dim()

ciInfo = ciWidth %>% 
  as_tibble() %>% # as_tibble is same than as.data.frame but faster
  pivot_longer(names(.)) %>% 
  bind_cols(grid) %>% select(-name) %>% rename(CIwidth = value)


ciInfo %>% ggplot(aes(correct, test, fill = CIwidth)) + geom_tile() +
  viridis::scale_fill_viridis()


# Confidence interval decrease with the size of the test set and the accuracy
```


# Part II Regression Models
## 5. Measuring Performance in Regression Models
```{r echo=FALSE}
#simulated data
observed <- c(0.22, 0.83, -0.12, 0.89, -0.23, -1.30, -0.15, -1.4, 0.62, 0.99, -0.18, 0.32, 0.34, -0.30, 0.04, -0.87, 0.55, -1.30, -1.15, 0.20)
predicted <- c(0.24, 0.78, -0.66, 0.53, 0.70, -0.75, -0.41, -0.43, 0.49, 0.79, -1.19, 0.06, 0.75, -0.07, 0.43, -0.42, -0.25, -0.64, -1.26, -0.07)

Residual_values = observed - predicted
Residual_values %>% summary()


# An important step in evaluating the quality of the model is to visualize the results

# 1) observed values against the predicted values helps one to understand how well the model fits.

plot(observed, predicted) ; abline(0, 1, col = "darkgrey", lty = 2)
plot(predicted, Residual_values, ylab = "residual") ; abline(h = 0, col = "darkgrey", lty = 2) ; 
# 2) Also, a plot of the residuals versus the predicted values can help uncover systematic patterns in the model predictions, such as trends


# calculating the RMSE and the R2 value
caret::RMSE(predicted, observed)
caret::R2(predicted, observed)

# 
# Simple correlation
cor(predicted, observed)
# Rank correlation
cor(predicted, observed, method = "spearman")
```

## 6. Linear Regression and Its Cousins
Import data for computing
```{r}
library(AppliedPredictiveModeling)
data(solubility)
## The data objects begin with "sol":
ls(pattern = "^solT")

train = solTrainY %>% bind_cols(solTrainXtrans) %>% rename(solubility = ...1)
test = solTestY %>% bind_cols(solTestXtrans) %>% rename(solubility = ...1)
```

Explore data
```{r}
head(solTestXtrans)
```

### LM Model
```{r}
lm = lm(solubility ~ ., data = train)
summary(lm)
# RMSE and R2 were 0.55 and 0.945, respectively.
# Note that these values are likely to be highly optimistic as they have been derived by re-predicting the training set data.

# Predict results of the test set
predictions = predict(lm, test)
predictions %>% head()

# Check performance 
Results = test %>% select(solubility) %>% bind_cols(predictions) %>% rename(pred = ...2) %>% rename(obs = solubility)
caret::defaultSummary(Results)

```
RLM Model
```{r}
# We are now going to do a robust linear regression 

rlm = MASS::rlm(solubility ~ ., data = train)
summary(rlm)


# Check model performance with cross-validation

cv = trainControl(method = "cv", number = 10)

set.seed(100)
lmfit = train(x = solTrainXtrans, y = solTrainY, 
                     method = "lm", trControl = cv)
lmfit
```
check model assumptions
```{r}
# residual distribution
  # Residuals versus the predicted values for the model 
plot(predict(lmfit), resid(lmfit), type = "p", xlab = "pred", ylab = "resid")
# we can see that there are no obvious warning signs in the diagnostic plots.
# The obs versus the predicted values : to assess how close the predictions are to the actual values. This plot can also show outliers or areas where the model is not calibrated.
Results %>% ggplot(aes(obs, pred)) + geom_point()
```
Rerun the model lm with pre-processing 
```{r}
# Pre-processing
train %>% glimpse()
tidy_rec = recipe(solubility ~ ., data = train) %>% 
  # BoxCox transformation is already done
  step_corr(all_numeric(), -all_outcomes(), threshold = 0.9) %>% 
  prep() 

tidy_rec %>% tidy(number = 1) # See what terms where remove (n = 38)

tidy_predata = tidy_rec %>% juice()

# Re-sampling

tidy_folds = vfold_cv(train, v = 10)

# Model

tidy_lm = linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")

# Worflow

tidy_wf = workflow() %>% 
  add_recipe(tidy_rec) %>% 
  add_model(tidy_lm)

# Fit with the folds

tidy_fit = tidy_wf %>% 
  fit_resamples(tidy_folds, metrics = metric_set(rmse, rsq))

# Final results of the lm 

collect_metrics(tidy_fit)
```

Rerun the model rlm with pre-processing 
```{r}

# However, it is important to note that rlm does not allow the covariance matrix of the predictors to be singular (unlike the lm function). To ensure that predictors are not singular, we will pre-process the predictors using PCA.
train %<>% mutate_all(., as.numeric) %>% glimpse()
tidy_rec = recipe(solubility ~ ., data = train) %>%
  step_corr(all_predictors(), threshold = 0.9) %>%
  step_pca(all_predictors(), num_comp = 10) # it's not recommended to applied PCA to categorical data
tidy_prep = tidy_rec %>% prep()

tidy_prep %>% tidy(2) %>% select(terms) %>% distinct()

tidy_predata = tidy_prep %>% juice()
```
```{r}
# Re-sampling
tidy_folds = vfold_cv(train, v = 10)
#####
# Create engine for rlm because it doesn't existe ####
set_model_engine("linear_reg", "regression", eng = "rlm")
set_dependency("linear_reg", eng = "rlm", pkg = "MASS")

set_fit(
  model = "linear_reg",
  eng = "rlm",
  mode = "regression",
  value = list(
    interface = "formula",
    protect = c("formula", "data", "weights"),
    func = c(pkg = "MASS", fun = "rlm"),
    defaults = list()
  )
)

set_pred(
  model = "linear_reg",
  eng = "rlm",
  mode = "regression",
  type = "numeric",
  value = list(
    pre = NULL,
    post = NULL,
    func = c(fun = "predict"),
    args =
      list(
        object = expr(object$fit),
        newdata = expr(new_data),
        type = "response"
      )
  )
)
#####

# Set model
tidy_rlm = linear_reg() %>% 
  set_engine("rlm")

# Set workflow
tidy_wf = workflow() %>% 
  add_recipe(tidy_rec) %>% 
  add_model(tidy_rlm) 

# Fit model
tidy_fit = tidy_wf %>% 
  fit_resamples(tidy_folds) # Error argument is of length zero (rlm is not implemented in tidy-models)

tidy_fit %>% collect_metrics()
```

### Partial least Squares 
```{r}

folds <- vfold_cv(v = 10, data = train)

tidy_rec <- recipe(solubility ~ ., data = train) %>% 
  step_center(all_numeric(), - all_outcomes()) %>% 
  step_scale(all_numeric(), - all_outcomes())

tidy_pls <- plsmod::pls(num_comp = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("mixOmics") 

tidy_wf <- workflow() %>% 
  add_recipe(tidy_rec) %>% 
  add_model(tidy_pls)

tidy_param <- tidy_wf %>% parameters() %>% 
  update(num_comp = num_comp(range = c(1, 30)))

tidy_tune <- tidy_wf %>% 
  tune_grid(
  resamples = folds,
  param_info = tidy_param,
  metrics = metric_set(rsq, rmse))

PLS <- tidy_tune %>% collect_metrics() %>% arrange(num_comp) %>% 
  filter(.metric == "rmse") %>% mutate(model = "PLS")
  

predict(tidy_pls, solTestXtrans[1:5,])
```
### PCA
```{r}
train %>% glimpse()
tidy_rec <- recipe(solubility ~ ., data = train) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_pca(all_predictors(), num_comp = tune())

tidy_lm <- linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")

tidy_wf <- workflow() %>% 
  add_recipe(tidy_rec) %>% 
  add_model(tidy_lm)

tidy_param <- tidy_wf %>% parameters() %>% 
  update(num_comp = num_comp(range = c(1, 30)))

tidy_tune <- tidy_wf %>% tune_grid(
  resample = folds,
  param_info = tidy_param,
  metrics = metric_set(rsq, rmse))

PCA <- tidy_tune %>% collect_metrics %>% arrange(num_comp) %>% 
  filter(.metric == "rmse") %>% mutate(model = "PCA")
```
Plot PCA vs PLS
```{r}
bind_rows(PCA, PLS) %>% 
  ggplot(aes(num_comp, mean, color = model)) + geom_line()
```

### Penalized regression models
```{r}
# We are going to perform an elasticnet regression

folds <- vfold_cv(v = 10, data = train)
tidy_rec <- recipe(solubility ~ ., data = train) %>% 
  step_normalize(all_numeric(), -all_predictors())

tidy_elast <- linear_reg(
  penalty = tune(),
  mixture = tune()) %>%
  set_mode("regression") %>% 
  set_engine("glmnet")



tidy_grid <- grid_regular(parameters(tidy_elast), levels = 5)

# tidy_wf <- workflow() %>% 
#   add_recipe(tidy_rec) %>% 
#   add_model(tidy_elast) 

# tidy_tune <- tidy_wf %>% tune_grid(
#   resamples = folds,
#   metrics = metric_set(rmse, rsq),
#   grid = tidy_grid)
metrics <- metric_set(rmse)
tidy_tune <- tune_grid(tidy_elast,
                        preprocessor = tidy_rec,
                        resamples = folds,
                        grid = tidy_grid,
                       metrics = metrics)


tidy_tune %>% pull(.notes)
tidy_tune %>% collect_metrics() %>% 
  filter(.metric == "rmse") %>% 
  ggplot(aes(penalty, mean, color = factor(mixture))) + geom_line()
```
```{r}
tidy_best <- tidy_tune %>% select_best("rmse")
linear_model <- finalize_model(tidy_elast, tidy_best)

tidy_last <- workflow() %>% 
  add_model(linear_model) %>% 
  add_recipe(tidy_rec)

library(vip)
tidy_last %>% fit(train) %>% pull_workflow_fit() %>% 
  vi() %>% mutate(Importance = abs(Importance)) %>% 
  mutate(Variable = fct_reorder(Variable, Importance)) %>%
  arrange(Importance) %>% 
  slice_head(n = 20) %>%
  ggplot(aes(Importance, Variable, fill = Sign)) + geom_col()


```


### Exercices chap 6

####Exo 6.1
```{r}
data(tecator)
```

PCA with the tidyverse
```{r}
tidy_rec <- recipe(~ ., data = absorp) %>% 
  step_normalize(all_numeric()) %>% # We need to normalize before PCA
  step_pca(all_numeric(), num_comp = ncol(absorp)) %>% prep() 

tidy_rec %>% tidy(2, type  = "variance") %>% mutate(component = as.factor(component)) %>%
  filter(terms == "variance") %>% 
  mutate(component = fct_reorder(component, as.numeric(component))) %>% 
  slice_head(n = 5) %>% 
  ggplot(aes(value, component)) + geom_col()



```
We should keep the 2 first pca

Data
```{r}
fat <- as.data.frame(endpoints) %>% dplyr::select(2) %>% rename(fat = V2)

data <- bind_cols(as.data.frame(absorp), fat) 

split <- initial_split(data, prop = 3/4)
train <- training(split)
test <- testing(split)

tidy_rec <- recipe(fat ~ ., data = train) %>% 
  step_normalize(all_predictors()) %>% 
  step_pca(all_predictors(), num_comp = 2) 

folds = vfold_cv(v = 10, repeats = 4, data = train)
```

PLS with the tidyverse => search best number of components
```{r}
fold <- vfold_cv(v = 5, data = train)
PLS_rec <- recipe(fat ~ ., data = train) %>% 
  step_normalize(all_predictors()) 

Pls_mod <- plsmod::pls(num_comp = tune()) %>% 
  set_engine("mixOmics") %>% 
  set_mode("regression")

tidy_grid = grid_regular(num_comp(range = c(1, 25)),
                         levels = 20)

PLS_tune <- Pls_mod %>% tune_grid(
  preprocessor = PLS_rec,
  resamples = fold,
  metrics = metric_set(rmse, rsq, mae),
  grid = tidy_grid)

PLS_tune %>% collect_metrics() %>% arrange(.metric, mean) %>% 
  filter(.metric == "rmse") %>% 
  ggplot(aes(num_comp, mean)) + geom_line()
```

Linear regression models 


Penalise regression
```{r}
tidy_rec <- recipe(fat ~ ., data = train) %>% 
  step_normalize(all_predictors()) %>% 
  step_pca(all_predictors(), num_comp = tune())

tidy_penal <- linear_reg(
  mixture = tune(),
  penalty = tune()
) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")

tidy_grid_model <- grid_regular(mixture(), 
                                penalty(),
                                num_comp(range = c(15, 20)),
                          levels = 3)
# tidy_grid <- grid_regular(parameters(tidy_penal), levels = 4) # Another way to do it

tidy_tune <- tidy_penal %>% tune_grid(
  preprocessor = tidy_rec,
  resamples = fold,
  grid = tidy_grid_model,
  metrics = metric_set(rsq, rmse, mae))

tidy_tune %>% collect_metrics() %>% 
  filter(.metric == "rmse") %>% 
  ggplot(aes(penalty, mean, color = factor(mixture))) + geom_line()

tidy_best <- tidy_tune %>% select_best("rmse")

Best_penal <- finalize_model(tidy_penal, tidy_best) # Best model is a pure lasso model with 20 PCA
Best_pca <- tidy_rec %>% finalize_recipe(tidy_best)
  
Last_wf <- workflow() %>% 
  add_recipe(Best_pca) %>% 
  add_model(Best_penal) 


# Evaluate the test set using the final model
Last_wf %>% last_fit(split) %>% collect_metrics()
```

PCR Lineal regression => we are going to search the number of ideal component (we saw in the first part that most of the variance is explain by the two first component but the model can really impove with more than those two component)
```{r}

lm_rec <- recipe(fat ~ ., data = train) %>% 
  step_normalize(all_predictors()) %>% 
  step_pca(all_predictors(), num_comp = tune())


tidy_lm <- linear_reg() %>% 
  set_mode("regression") %>% 
  set_engine("lm")

tidy_grid <- grid_regular(num_comp(range = c(1, 20)),
                                   levels = 20)

Tuning_lm <- tidy_lm %>% tune_grid(
  preprocessor = lm_rec,
  grid = tidy_grid,
  resamples = fold,
  metrics = metric_set(rmse, rsq, mae)
) 

Tuning_lm %>% collect_metrics() %>% filter(.metric == "rmse") %>% 
  ggplot(aes(num_comp, mean)) + geom_line()
  
Best_tuning <- Tuning_lm %>% select_best()

Final_rec <- lm_rec %>% finalize_recipe(Best_tuning)


Lm_wf <- workflow() %>% 
  add_recipe(Final_rec) %>% 
  add_model(tidy_lm)

Lm_wf %>% last_fit(split) %>% collect_metrics()

  
```
Perform a PLS instead of a PCR
```{r}
PLS_rec <- recipe(fat ~ ., data = train) %>% 
  step_normalize(all_predictors()) %>% 
  step_pls(all_predictors(), outcome = "fat", num_comp = tune()) 

# We use lm as in the last model 
pls_grid <- grid_regular(num_comp(range = c(1,3)),
                         levels = 3)

tidy_tune <- tune_grid(tidy_lm,
  preprocessor = PLS_rec,
  resamples = folds,
  grid = pls_grid,
  metrics = metric_set(rmse, rsq)
) # error

tidy_best <- tidy_tune %>% select_best("rmse") # Best 3 num_comp
Final_rec <- PLS_rec %>% finalize_recipe(tidy_best)

lm_wf <- workflow() %>% 
  add_recipe(Final_rec) %>% 
  add_model(tidy_lm)
    
lm_wf %>% last_fit(split) %>% collect_metrics()
```
The model with the best predictive ability is hard to say in this case because i use different parameter for tuning the models so this need to be redone. 


#### Exo 6.2
a) Import data
```{r}
data("permeability")

data <- bind_cols((as.data.frame(fingerprints)), as.data.frame(permeability)) 

split <- initial_split(data)
train <- training(split)
test <- testing(split)

# Data exploration (if we want to apply a linear model we need to check if the data is normal and not skewed)
data %>% ggplot(aes(permeability)) + geom_histogram()
# We can clearly see that the data is skewed we could realize a boxcox transformation or log 
data %<>% mutate(permeability = log(permeability)) %>% 
  ggplot(aes(permeability)) + geom_histogram()
# It's much better
```
b) Preprocessing check near zero variance
```{r}
tidy_rec <- recipe(permeability ~ ., data = data) %>% 
  step_nzv(all_predictors())

tidy_rec %>% prep() %>% tidy(1)
```
After data processing we have 719 predictors. The high number of predictors that are remove indicates that many of the ngerprints are
describing unique features of very small subsets of molecules.

c) PLS
```{r}
tidy_rec <- recipe(permeability ~ ., data = train) %>% 
  step_nzv(all_predictors()) %>% 
  step_log(all_outcomes())

tidy_PLS <- plsmod::pls(num_comp = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("mixOmics")

folds <- vfold_cv(v = 10, data = train) # we probably shoud use cv-folds because of the small sample size 

tidy_grid <- grid_regular(num_comp(range = c(1, 40)),
                          levels = 20)

tidy_tuning <- tidy_PLS %>% tune_grid(
  preprocessor = tidy_rec,
  grid = tidy_grid,
  metrics = metric_set(rmse, rsq, mae),
  resamples = folds)


tidy_tuning %>% collect_metrics() %>% filter(.metric == "rsq") %>% 
  ggplot(aes(num_comp, mean)) + geom_line()

tidy_best <- tidy_tuning %>% select_best("rsq")
Final_PLS <- tidy_PLS %>% finalize_model(tidy_best)


Last_wf <- workflow() %>% 
  add_recipe(tidy_rec) %>% 
  add_model(Final_PLS)


```
The optimal number of components is 5.
Lets now plot them against the log(permeability)
```{r}
PLS <- recipe(permeability ~ ., data = data) %>% 
  step_nzv(all_predictors()) %>% 
  step_pls(all_predictors(), outcome = "permeability", num_comp = 5) %>% 
  prep() %>% juice()

PLS %>% ggplot(aes(permeability, PLS1)) + geom_point() + geom_smooth(method = "lm") + geom_smooth(method = "loess")
```
The relationship between the PLS components and the response can be seen in Figure 6, where the straight line is the linear regression fit, and the curved line is the loess fit. For the training data, the first three components are linearly related to log10(permeability). Each component also contains a noticeable amount of noise relative to predicting the response.

d) Predict the response on the test set
```{r}
# Realize predictions the test set we firsts need to fit the model to the train data
Fit_PLS <- Final_PLS %>% fit(permeability ~ ., data = tidy_rec %>% prep() %>% juice())

Pred_test <- Fit_PLS %>% predict(tidy_rec %>% prep() %>% bake(test))# Predict in the pre-processed data

Prep_test <- tidy_rec %>% prep() %>% bake(test) %>% select(permeability)# Preproces the test data for plotting
Pred_test %>% bind_cols(Prep_test) %>% ggplot(aes(.pred, permeability)) + geom_point() + geom_smooth()

# Evaluate the model train with a leave one out cross validation
LGOCV <- loo_cv(train) # Not available in tidymodels for now!!!
Folds_rep <- vfold_cv(v = 10, repeats = 1, data = train)

Eval_PLS_folds <- Final_PLS %>% fit_resamples(permeability ~ .,
                                Folds_rep)

Eval_PLS_folds %>% collect_metrics()

# Finally evaluate the model in the test data 
Tidy_fit <- Last_wf %>% last_fit(split) %>% collect_metrics()


```
The test set estimate of Rsq is 0.50 and 0.49 when we evaluate the model with cross-validation. 

e) The model prediction doesn't look good so we are going to try a penalized model 

```{r}
tidy_rec <- recipe(permeability ~ ., data = train) %>% 
  step_log(all_outcomes()) %>% 
  step_nzv(all_predictors()) 

tidy_LRP <- linear_reg(mixture = tune(),
                       penalty = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")

tidy_grid <- expand.grid(mixture = c(0, 0.01, 0.1),
                          penalty = seq(from = 0.01, to = 1, by = 0.1))

folds <- vfold_cv(v = 5, repeats = 1, data = train) # We should use repeat 5 and v = 10 due to the sample size 

Tidy_tuning <- tidy_LRP %>% tune_grid(
  preprocessor = tidy_rec,
  resamples = folds,
  metrics = metric_set(rmse, mae, rsq), # error with rsq -> solve
  grid = tidy_grid
)

Tidy_tuning %>% collect_metrics() %>% filter(.metric == "rsq") %>% 
  ggplot(aes(penalty, mean, color = factor(mixture))) + geom_line()

Tidy_best <- Tidy_tuning %>% select_best("rsq")
# Best tuning penalty = 1e-10, mixture = 1 (lasso regression)

Final_model <- tidy_LRP %>% finalize_model(Tidy_best)

Final_wf <- workflow() %>% 
  add_model(Final_Lasso) %>% 
  add_recipe(tidy_rec)

Final_wf %>% last_fit(split) %>% collect_metrics()
```
rsq = 0.46 we didn't improve the model with the ridge regression we should try PLS and elastic net
```{r}
# RMSE has the same unit as the dependent variable so we check how good is our RMSE (Range = 0 - 55) so 1.1 seems ok.
train %>% select(permeability) %>% summary()
```
Let's try to impove the predictions
```{r}
# PLS and elastic net
library(mixOmics)
tidy_rec <- recipe(permeability ~ ., data = train) %>% 
  step_log(all_outcomes()) %>% 
  step_nzv(all_predictors()) %>% 
  step_pls(all_predictors(), outcome = "permeability", num_comp = tune()) 


Tidy_model <- linear_reg(mixture = tune(),
                         penalty = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")

tidy_grid <- expand.grid(mixture = c(0, 0.01, 0.1, 1),
                         penalty = seq(from = 0.01, to = 0.5, by = 0.1),
                         num_comp = c(5, 8, 10))

folds <- vfold_cv(v = 3, repeats = 1, data = train)
tidy_tune <- Tidy_model %>% tune_grid(
  preprocessor = tidy_rec,
  grid = tidy_grid,
  resamples = folds,
  metrics = metric_set(mae, rmse, rsq)
)
tidy_tune %>% collect_metrics()
tidy_best <- tidy_tune %>% select_best("rmse")


Final_rec <- tidy_rec %>% finalize_recipe(tidy_best)
Final_mod <- Tidy_model %>% finalize_model(tidy_best)

tidy_wf <- workflow() %>% 
  add_recipe(Final_rec) %>% 
  add_model(Final_mod) 
  
tidy_wf %>% last_fit(split) %>% collect_metrics()

```
The model doesn't improve it's probably a tuning problem and we need to use a bigger grid. We should use the model for prediction wich has the best rsq and RMSE

#### Exo 6.3
Understand the relationship between biological measurements of the raw materials
```{r}
data(ChemicalManufacturingProcess)
split <- ChemicalManufacturingProcess %>% initial_split(prop = 0.7)
train <- training(split)
test <- testing(split)
```

a) Imputation
```{r}
# Check data
dim(ChemicalManufacturingProcess)
ChemicalManufacturingProcess %>% ggplot(aes(Yield)) + geom_histogram() # Looks symetric so doesn't need transformation prior to analysis
colSums(is.na(ChemicalManufacturingProcess)) %>% stack() %>% arrange(-values) #Since no rows or columns have too much missing data, we can impute this information without perturbing the true underlying relationship between the predictors and the response.Specifically, 106 cells are missing out of 10032 or 1.06 percent

# Are there any univariate relationships with the response? Because we have a small number of predictors, we can explore these relationships visually with a correlation plot
ChemicalManufacturingProcess %>% GGally::ggcorr()
# many of the biological material predictors are highly positively correlated, and some of the manufacturing process predictors are positively and negatively correlated.
```

```{r}
tidy_rec <- recipe(Yield ~ ., data = train) %>% 
  step_knnimpute(all_predictors()) %>% 
  step_BoxCox(all_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_corr(all_predictors())

tidy_rec %>% prep() %>% juice() -> prepro_data ; colSums(is.na(prepro_data)) ; dim(prepro_data) ; rm(count_na) # Imputation worked

# From book solutions: Given the moderate pairwise correlations that were apparent in Figure 11, a dimension reduction or shrinkage technique would be an appropriate model for this data.

boots <- bootstraps(time = 25, data = train)

tidy_model <- plsmod::pls(num_comp = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("mixOmics")

tidy_grid <- expand.grid(num_comp = seq(from = 1, to = 48, by = 5))

tidy_tune <- tidy_model %>% tune_grid(
  preprocessor = tidy_rec,
  grid = tidy_grid,
  resamples = boots,
  metrics = metric_set(mae, rmse, rsq)
)

tidy_tune %>% collect_metrics() %>% filter(.metric == "rsq") %>% 
  ggplot(aes(num_comp, mean)) + geom_line()
```
c)
```{r}
tidy_best <- tidy_tune %>% select_best("rsq")
Final_model <- tidy_model %>% finalize_model(tidy_best)
```

The best model is when we use 6 components 

d)
```{r}
tidy_wf <- workflow() %>% 
  add_model(Final_model) %>% 
  add_recipe(tidy_rec) 

# Eval on bootstrap training data
tidy_eval <- tidy_wf %>% fit_resamples(boots, metrics = metric_set(rmse, rsq, mae))
tidy_eval %>% collect_metrics()
# Eval on test data 
last_fit <- tidy_wf %>% last_fit(split, metrics = metric_set(rmse, rsq, mae))
last_fit %>% collect_metrics()

predictions <- last_fit %>% collect_predictions()
Fit_PLS <- tidy_wf %>% fit(data = train)
Pred_test <- Fit_PLS %>% predict(new_data = test)# Predict in the pre-processed data
predictions %>% ggplot(aes(Yield, .pred)) + geom_point() + geom_smooth(method = "lm") # easier way
waldo::compare(Pred_test, (predictions %>% select(.pred))) # Check if it is the same 
```

Evaluation in train bootstrap: rmse = 1.988107, rsq = 0.379704
Evaluation in test set: rmse = 1.7116411, rsq = 0.3795456

Pretty similar results

e)
```{r}
# check the most important predictors
tidy_info <- Fit_PLS %>% pull_workflow_fit()
str(tidy_info$fit$explained_variance)
str(tidy_info$fit$variates$X)
loadings <- tidy_info$fit$loadings$X


# See how much each predictor contribute to each component
library(vip)
loadings %>% as.data.frame() %>% rownames_to_column() %>% #vi() #Model-specific variable importance scores are currently not available for this type of model.
  select(rowname, comp1, comp2, comp3, comp4, comp5) %>% 
  pivot_longer(-rowname) %>% 
  ggplot(aes(value, rowname)) + geom_col() + facet_wrap(~ name, nrow = 1)


# PLS variable importance
tidy_load <- loadings %>% as.data.frame() %>% rownames_to_column() %>% 
  select(rowname, comp1, comp2, comp3) %>% 
  pivot_longer(-rowname) %>% 
  rename(predictors = rowname)

tidy_load %>% mutate(Sing = if_else(value < 0, "neg", "pos")) %>% 
  mutate(absvalue = abs(value)) %>% group_by(predictors) %>% summarise(Importance = sum(absvalue)) %>% 
  mutate(predictors = fct_reorder(predictors, Importance)) %>% 
  slice_head(n = 15) %>% 
  ggplot(aes(Importance, predictors, fill = predictors)) + geom_col(show.legend = F) 

# or use 

tidy(Fit_PLS)
  
```
The most important predictors are mostly material (this results contradict the solution maybe this is the wrong way to check variable importance?)

f)

Let's explore the relationships between the three top important predictors and the response.
```{r}
tidy_rec %>% prep() %>% juice() %>% 
  ggplot(aes(ManufacturingProcess09, Yield)) + geom_point() + geom_smooth(method = "loess")
tidy_rec %>% prep() %>% juice() %>% 
  ggplot(aes(ManufacturingProcess13, Yield)) + geom_point() + geom_smooth(method = "loess")
tidy_rec %>% prep() %>% juice() %>% 
  ggplot(aes(ManufacturingProcess32, Yield)) + geom_point() + geom_smooth(method = "loess")



```

Clearly Process 9 and Process 32 have a positive relationship with Yield, while
Process 13 has a negative relationship. If these manufacturing processes can be controlled, then
altering these steps in the process to have higher (or lower) values could improve the overall Yield of
the process. A statistically designed experiment could be used to investigate a causal relationship
between the settings of these processes and the overall yield.
